\documentclass[11pt,english]{article}

\setlength\parindent{0pt}
\usepackage{amssymb,amsmath,amscd,graphicx,fontenc,bbold,bm,amsthm,mathrsfs,mathtools}

\usepackage{algorithm, algorithmic}

% code and psendocode
\usepackage{verbatim}
\usepackage{minted}
\usemintedstyle{trac}

% colors
\usepackage{xcolor}
\definecolor{LightGray}{gray}{0.95}
\definecolor{DarkGray}{gray}{0}

% new commands
\newcommand{\code}[1]{{\small\colorbox{LightGray}{\texttt{#1}}}}
\newcommand{\tr}{\mathrm{Trace}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\sigmoid}{\mathrm{sigmoid}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\abs}{\mathrm{abs}}
\newcommand{\rect}{\mathrm{rect}}
\newcommand{\onehot}{\mathrm{onehot}}
\newcommand{\relu}{\mathrm{relu}}
\newcommand{\pd}[1]{\frac{\partial}{\partial {#1}}}

\usepackage[top=3cm,bottom=3cm,right=2.3cm,left=2.3cm,twoside=false]{geometry}

\title{IFT6390-fundamentals of machine learning\\Assignment 3}
\author{Jonathan Guymont, Marzieh Mehdizadeh}

\date{}

\begin{document}

\maketitle

\begin{equation}
\begin{split}
x
\end{split}
\end{equation}

\section*{Question 1}
(a) 
\begin{equation}
\begin{split}
0.5(\tanh(0.5x)+1) =& 0.5 (\frac{e^{0.5x}-e^{-0.5x}}{e^{0.5x}+e^{-0.5x}}+1)\\
=& 0.5 (\frac{1-e^{-x}}{1+e^{-x}}+\frac{1+e^{-x}}{1+e^{-x}})\\
=& 0.5\frac{2}{1+e^{-x}}\\
=& \frac{1}{1+e^{-x}}\\
\end{split}
\end{equation}

(b) 
\begin{equation}
\begin{split}
\log \sigmoid(x)=& \log (1+e^{-x})^{-1}\\
=& - \log (1+e^{-x})\\
=& -\softmax(-x)
\end{split}
\end{equation}

(c)

\begin{equation}
\begin{split}
\frac{d}{dx}\sigmoid(x) =& \frac{d}{dx} (1+e^{-x})^{-1}\\
=& -(1+e^{-x})^{-2}\frac{d}{dx} 1+e^{-x} \\
=& (1+e^{-x})^{-2}e^{-x}\\
=& (1+e^{-x})^{-1}\frac{e^{-x}}{1+e^{-x}}\\
=& (1+e^{-x})^{-1}(\frac{1+e^{-x}-1}{1+e^{-x}})\\
=& (1+e^{-x})^{-1}(1- \frac{1}{1+e^{-x}})\\
\end{split}
\end{equation}

(4) 
\begin{equation}
\begin{split}
\tanh'(x)=& (\frac{e^x-e^{-x}}{e^x+e^{-x}})'\\
=& \frac{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}{(e^x+e^{-x})^2}\\
=& \frac{(e^x+e^{-x})^2-(e^x-e^{-x})^2}{(e^x+e^{-x})^2}\\
=& 1- \frac{(e^x-e^{-x})^2}{(e^x+e^{-x})^2}\\
=& 1 - \tanh^2(x)
\end{split}
\end{equation}

(5) $\sign(x) = -1 + 2\cdot \mathbb{1}_{x>0}$ \\

(6) $\abs'(x)=\sign(x)$\\

(7) $\rect'(x) = \mathbb{1}_{x>0}$ \\

(8) Let $f(\bm{x})=\sum_{x_i \in \bm{x}} x_i^2$, then $f'(\bm{x})=(f'_{x_1},...,f'_{x_{|\bm{x}|}})=(2x_1,...,2x_{|\bm{x}|})$\\

(9) Let $f(\bm{x})=\sum_{x_i \in \bm{x}} |x_i|$, then $f'(\bm{x})=(f'_{x_1},...,f'_{x_{|\bm{x}|}})=(\sign(x_1),...,\sign(x_{|\bm{x}|}))$\\

\section*{Gradient Computation for Parameters optimizations in a neural net for multiclass classification}

(1) The dimension of $\bm{b}^{(1)}$ is $d_h \times 1$. The formula of the preactivation vector is 
\[
	\bm{h}^a = \bm{W}^{(1)}\bm{x} + \bm{b}^{(1)} 
\]
and the formula for obtaining the value of the element $j$ is
\[
	\bm{h}_j^a = \bm{W_{j,\cdot}}^{(1)}\bm{x} + \bm{b}_j^{(1)}
	=\bm{b}_j^{(1)} + \sum_{i=1}^d \bm{W_{j,i}}^{(1)}\bm{x}_i.
\]
The output vector of the activation is given by
\[
	\bm{h}^s = \relu(\bm{h}^a)
\]
where $\relu(\cdot)$ is applied element wise, i.e. $\bm{h}_j^s = \max(0, \bm{h}_j^a)$, $j=1,...,d_h$.\\

(2) The dimension of $\bm{W}^{(2)}$ is $m \times d_h$ and the dimension of $\bm{b}^{(2)}$ is $m \times 1$. 
\[
	\bm{o}^a = \bm{W}^{(2)}\bm{h}^s + \bm{b}^{(2)}
\]
\[
	\bm{o}_k^a 
	= \bm{W}_{k,\cdot}^{(2)}\bm{h}^s + \bm{b}_k^{(2)}
	= \bm{b}_k^{(2)} + \sum_{i=1}^{d_h} \bm{W_{k,i}}^{(2)}\bm{h}^s_i
\]
for $k=1,..,m$.\\

(3) 
\begin{equation}
	\bm{o}^s_k = \frac{\exp(\bm{o}_k^a)}{\sum_{k=1}^m \exp(\bm{o}_k^a)}
	\label{eq:softmax}
\end{equation}
They are all positive because $\exp\colon \real \mapsto \real^+$. Also a sum of positive number is positive. And the ratio of a posive number over a positive number is also positive.
\[
	\sum_{k=1}^m \bm{o}^s_k = \frac{1}{\sum_{k=1}^m \exp(\bm{o}_k^a)} \sum_{k=1}^m \exp(\bm{o}_k^a)=1  	
\]

(4) Let $Z=\sum_{k=1}^m \exp(\bm{o}_k^a)$. Then $\bm{o}^s=\frac{1}{Z}(\exp(\bm{o}_1^a),...,\exp(\bm{o}_m^a))^\top$ and 
\[
	L(\bm{x}, y) = -\log \onehot_m(y) (\exp(\bm{o}_1^a(\bm{x}))/Z,...,\exp(\bm{o}_m^a(\bm{x}))/Z)^\top
	= -\log \onehot_m(y)\bm{o}^s(\bm{x})
\]
where $\onehot_m(y)$ is a $1 \times  m$ onehot representation for $y$.\\
 
(5) $\hat{R}$ is an estimation of the expected value of the loss function (minus the loglikelihood in our case)
\[
	\hat{R} = \frac{1}{n}\sum_{i=1}^n L(x^{(i)}, y^{(i)})=\frac{1}{n}\sum_{i=1}^n 
	-\log \onehot_m(y^{(i)})\bm{o}^s(\bm{x}^{(i)})
\]
The set of trainable parameters is $\bm{\theta}=\{W^{(1)}, W^{(2)}, b^{(1)}, b^{(2)}\}$. The number of scalar parameters is $n_\theta=d_h\cdot d + d_h + m\cdot d_h + m=d_h(d+1)+m(d_h+1)$. \\

\textbf{Optimization problem.} First we need to initialize the parameters properly. To find the parameters that minimize the loss, we need to compute the derivative of the loss function w.r.t each parameters. Then we update each parameters by moving them in the opposite direction of their gradient (since we minimize). We repeat this step until a stopping criterion is met (e.g. maximum number of iteration is reached when using early stopping).\\

(6)
\begin{algorithm}[H]
	\begin{algorithmic}
		\REQUIRE Step size $\eta$
		\REQUIRE Initial parameter $\bm{\omega}_0$
		\REQUIRE Number of iterations $T$
		\FOR{$i=1$ to $T$}
		\STATE Compute gradient $\bm{g}_t=\frac{1}{m}\nabla_{\bm{\omega}}\sum_i L(\bm{x}^{(i)}, \bm{y}^{(i)})$ 
		\STATE Apply update: $\bm{\omega}_t=\bm{\omega}_{t-1}-\eta \bm{g}_t$
		\ENDFOR
	\end{algorithmic}
	\caption{Pseudocode for Batch Gradient Descent}
	\label{alg:seq}
\end{algorithm}

(7) 

\begin{equation}
\begin{split}
	\pd{\bm{o}^a}L
	=& \pd{\bm{o}^a} -\log \onehot_m(y)\bm{o}^s\\
	=& -\frac{1}{\onehot_m(y)\bm{o}^s}\pd{\bm{o}^a} \onehot_m(y)\bm{o}^s \\
	=& -\frac{1}{\onehot_m(y)\bm{o}^s}\pd{\bm{o}^a} \onehot_m(y)\softmax(\bm{o}^a) \\ 
	=& -\frac{1}{\onehot_m(y)\bm{o}^s}\onehot_m(y) \softmax(\bm{o}^a)(\onehot_m(y)-\softmax(\bm{o}^a)) \\
	=& - (\onehot_m(y)-\softmax(\bm{o}^a))
\end{split}
\end{equation}

(8) 

\begin{minted}[bgcolor=LightGray, fontsize=\footnotesize]{python}
import numpy as np

def grad_oa(x, y):
    onehot = np.zeros(m)
    onehot[y] = 1
    return os(x) - onehot
\end{minted}

\section*{Neural Network Implementation and Experiment}



\end{document}